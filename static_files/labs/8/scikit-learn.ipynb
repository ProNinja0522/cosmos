{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e228639a",
   "metadata": {},
   "source": [
    "# Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94a31a5",
   "metadata": {},
   "source": [
    "Main features of `scikit-learn`:\n",
    "\n",
    "- **Preprocessing**: scaling, encoding, missing value imputation, ...\n",
    "- **Supervised learning**: regression, classification, ...\n",
    "- **Unsupervised learning**: clustering, dimension reduction, ...\n",
    "- **Model evaluation**: measuring model performance based on some metrics.\n",
    "- **Model selection**: choosing the best model or set of hyperparameters based on performance on validation data.\n",
    "- **Pipeline and compose** (will not be covered here)\n",
    "\n",
    "The fundamental object in `scikit-learn` is `BaseEstimator`, and the heierarchy of the objects are as follows:\n",
    "```text\n",
    "BaseEstimator\n",
    "├── TransformerMixin      ← Data Transformation (preprocessing, dimension reduction, ...)\n",
    "├── ClassifierMixin       ← Estimator (classification)\n",
    "├── RegressorMixin        ← Estimator (regression)\n",
    "├── ClusterMixin, etc.    ← Estimator (clustering)\n",
    "└── ...\n",
    "```\n",
    "\n",
    "In what follows, I will go over various objects/features on `scikit-learn` by following the standard workflow of ML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de26167",
   "metadata": {},
   "source": [
    "## 0. Data Split\n",
    "- In ML, it's important to evaluate how well a model performs on unseen (or new) data. \n",
    "- To do this, we split the dataset into a training set (for fitting the model) and a test set (for final evaluation). Later, we will also split the training set (validation, cross-validation) for a model selection.\n",
    "- After this step, you should **not** use any information about the test set.\n",
    "- Before splitting the data, we assume that any transformations that do not depend on the training data — such as **type conversion** or **dropping rows with missing values** — have already been applied.\n",
    "\n",
    "In `scikit-learn`, the `train_test_split` function from the `sklearn.model_selection` module is used to randomly split the data into training and test sets based on the `test_size` parameter. Setting the `random_state` ensures that the split is reproducible.\n",
    "\n",
    "\n",
    "```python\n",
    "# Code template for data split\n",
    "X = DataFrame_of_predictors\n",
    "y = DataFr\n",
    "from sklearn.model_selection import train_test_split\n",
    "ame_of_responses # Ignore if unsupervised\n",
    "\n",
    "TEST_PROPORTION = 0.2 # any float between 0 and 1\n",
    "SEED = 100 # any integer\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=TEST_PROPORTION, random_state=SEED)\n",
    "\n",
    "# For unsupervised learning,\n",
    "X_train, X_test = train_test_split(X, test_size=0.2, random_state=SEED)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f364c5",
   "metadata": {},
   "source": [
    "## 1. Preprocessing\n",
    "- After splitting the data into training and test sets, we apply preprocessing steps such as:\n",
    "  -  imputing missing values\n",
    "  -  scaling numerical features\n",
    "  -  encoding categorical variables.\n",
    "- It is important that all transformations that learn from the data — like calculating means or encoding mappings — are fit only on the training set. (WHY?)\n",
    "- Once fit, these transformers should then be applied to both the training and test data.\n",
    "- We use `TransformerMixin` object for preprocessing.\n",
    "\n",
    "**Examples**\n",
    "\n",
    "| Transformer           | Description                                                 | Typical Use Case                       |\n",
    "| --------------------- | ----------------------------------------------------------- | -------------------------------------- |\n",
    "| `SimpleImputer`       | Fills in missing values with mean, median, or most frequent | Handling missing data                  |\n",
    "| `StandardScaler`      | Standardizes features (zero mean, unit variance)            | Scaling numeric features               |\n",
    "| `MinMaxScaler`        | Scales features to a given range (default: \\[0, 1])         | Normalizing data for bounded models    |\n",
    "| `RobustScaler`        | Scales using median and IQR (robust to outliers)            | Scaling with outliers                  |\n",
    "| `OneHotEncoder`       | Converts categorical variables into binary columns          | Encoding nominal categorical features  |\n",
    "| `OrdinalEncoder`      | Encodes categories as ordered integers                      | Encoding ordinal categorical features  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16393de",
   "metadata": {},
   "source": [
    "Suppose you want to transform your dataframe `df` with the specific `Transformer`.\n",
    "\n",
    "```python\n",
    "# Code template for TransformerMixin object\n",
    "df = DATA\n",
    "tf = Transformer(...)               # Instantiation\n",
    "tf.fit(df)                          # Fitting\n",
    "df_transformed = tf.transform(df)   # Transforming\n",
    "```\n",
    "---\n",
    "\n",
    "Fitting and transformin can be done simultaneously by `fit_transform()`\n",
    "```python\n",
    "# Code template for TransformerMixin object (fit + transform)\n",
    "tf = Transformer(...)                   # Instantiation\n",
    "df_transformed = tf.fit_transform(df)   # Fitting + transforming\n",
    "```\n",
    "The object `tf` automaically saves the fit.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "## Which one is wrong?\n",
    "X = pd.DataFrame(...)\n",
    "X_train, X_test = train_test_split(X, test_size = 0.2, random_state = 87)\n",
    "\n",
    "### (1) ###\n",
    "tf = Transformer(...)\n",
    "tf.fit(X_train)\n",
    "X_train_transformed = tf.transform(X_train)\n",
    "X_test_transformed = tf.transform(X_test)\n",
    "\n",
    "### (2) ###\n",
    "tf = Transformer(...)\n",
    "tf.fit(X)\n",
    "X_train_transformed = tf.transform(X_train)\n",
    "X_test_transformed = tf.transform(X_test)\n",
    "\n",
    "### (3) ###\n",
    "tf = Transformer(...)\n",
    "tf.fit(X_train)\n",
    "X_train_transformed = tf.transform(X_train)\n",
    "X_test_transformed = tf.transform(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da706387",
   "metadata": {},
   "source": [
    "### Code Examples with Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c113200",
   "metadata": {},
   "source": [
    "#### 1.1 Missing value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7174fba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy = 'mean') # strategy: mean, most_frequent, median, constant, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669a22c",
   "metadata": {},
   "source": [
    "#### 1.2 Scaling (Numerical variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e4ccfc",
   "metadata": {},
   "source": [
    "When a model depends on distance calculations or involves gradient-based optimization, feature scaling becomes essential. Examples include:\n",
    "| **Model**                        | **Reason**                                              |\n",
    "| -------------------------------- | ---------------------------------------------------------------------- |\n",
    "| `LogisticRegression`             | Optimization assumes features are on a similar scale                   |\n",
    "| `Ridge`, `Lasso`, `ElasticNet`   | Regularization penalties are sensitive to feature magnitude            |\n",
    "| `SVM           `                 | Distance-based; feature scales affect margin and kernel behavior       |\n",
    "| `KNeighborsClassifier/Regressor` | Distance-based; dominant features distort results                      |\n",
    "| `KMeans`                         | Uses Euclidean distance; scale mismatch skews clustering               |\n",
    "| `PCA`                            | Based on variance; larger-scale features dominate principal components |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "304f2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6ff42d",
   "metadata": {},
   "source": [
    "#### 1.3 Dimensionality Reduction (Numerical variable)\n",
    "- Even though dimensionality reduction techniques like PCA or t-SNE themselves are unsupervised learning models, they are often applied during preprocessing to reduce noise, remove redundancy, or simplify the feature space.\n",
    "- For example, PCA can help improve model performance or training speed by projecting high-dimensional data onto a smaller set of orthogonal components.\n",
    "- Each dimensionality reduction object is also a subclass of `TransformerMixin`, so they share the same code template.\n",
    "- However, just like other data-dependent transformations, dimensionality reduction should be fit only on the training data and then applied to the test data.\n",
    "\n",
    "Since we only have two numerical varialbes in the dataset, we do not pursue this direction in our preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5218050",
   "metadata": {},
   "source": [
    "#### 1.4 Encoding (Categorical variable)\n",
    "Encoding for categorical variables is necessary for any model that requires numerical input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12721958",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "encoder = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88adf897",
   "metadata": {},
   "source": [
    "#### 1.5 Save"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f546c49",
   "metadata": {},
   "source": [
    "After preprocessing, save \n",
    "- `X_train_processed`, `X_test_processed`, `y_train`, `y_test` in `data/processed` by using `to_csv()`.\n",
    "- all the transformers you used (`imputer`, `scaler`, `encoder`) in `model` by using `pickle`.\n",
    "\n",
    "It's better to save the processed datasets and transformers separately for each model, especially when different models require different preprocessing steps. That is, save\n",
    "- `X_train_processed`, `X_test_processed`, `y_train`, `y_test` in `data/processed/ModelName` by using `to_csv()`\n",
    "- all the transformers you used (`num_imputer`, `cat_imputer`, `scaler`, `encoder`) in `model/ModelName` by using `pickle`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc3c7fb",
   "metadata": {},
   "source": [
    "#### IMPORTANT NOTE\n",
    "- In addition to scikit-learn’s transformers, many preprocessing steps are often performed manually using pandas or custom logic.\n",
    "- These include creating new features, grouping categories, handling outliers, or applying domain-specific transformations.\n",
    "- You can perform such steps at any point in your workflow, as long as you ensure they are applied consistently to both training and test sets.\n",
    "- Furthermore, as you conduct exploratory data analysis (EDA), you may identify patterns, anomalies, or relationships that reveal the need for additional preprocessing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32de0a4",
   "metadata": {},
   "source": [
    "## 2. Fitting and Prediction\n",
    "- Fitting a model means finding the best parameters that minimize the error between the model's predictions and the true values on the training data.\n",
    "- Once the model is fit on the training data, it can be used to make predictions on test data.\n",
    "\n",
    "**Supervised learning**\n",
    "\n",
    "Let's say you want to fit a supervised model on (`X_train`, `y_train`), and use it to predict outcomes for `X_test`. (Here, I assume the data are processed).\n",
    "\n",
    "```python\n",
    "# Code template for supervised model\n",
    "model = ModelName(...)              # Instantiation. You should import `ModelName` first.\n",
    "model.fit(X_train, y_train)         # Fitting\n",
    "y_pred = model.predict(X_test)      # Prediction\n",
    "```\n",
    "\n",
    "\n",
    "**Unsupervised learning**\n",
    "\n",
    "Let's say you want to fit an unsupervised model on `X_train`, and then use it to transform (dimension reduction) or assign label (clustering) to `X_test`. (Here, I assume the data are processed.)\n",
    "\n",
    "```python\n",
    "# Code template for dimension reduction model\n",
    "model = ModelName(...)                        # Instantiation. You should import `ModelName` first.\n",
    "model.fit(X_train)                            # Fitting\n",
    "X_train_reduced = model.transform(X_train)    # Dimension reduction for training set\n",
    "X_test_reduced = model.transform(X_test)      # Dimension reduction for test set\n",
    "```\n",
    "\n",
    "```python\n",
    "# Code template for clustering model\n",
    "model = ModelName(...)                    # Instantiation. You should import `ModelName` first.\n",
    "model.fit(X_train)                        # Fitting\n",
    "train_labels = model.labels_              # Cluster assignment for training set\n",
    "test_labels = model.predeict(X_test)      # Cluster assignment for test set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30404bf",
   "metadata": {},
   "source": [
    "## 3. Model Evaluation\n",
    "- Model evaluation is the process of measuring how well a trained model performs on both the training data and test data.\n",
    "- Comparing training and test performance helps identify issues such as overfitting or underfitting.\n",
    "- Here, we only consider the evaluation for supervised learning.\n",
    "\n",
    "**Evaluation metrics for regression (Examples)**\n",
    "| **Metric**                            | **Description**                          | **scikit-learn Function**                |\n",
    "| ------------------------------------- | ---------------------------------------- | ---------------------------------------- |\n",
    "| Mean Absolute Error (MAE)             | Average of absolute errors               | `mean_absolute_error`                    |\n",
    "| Mean Squared Error (MSE)              | Average of squared errors                | `mean_squared_error`                     |\n",
    "| Root Mean Squared Error (RMSE)        | Square root of MSE (same unit as target) | `mean_squared_error(..., squared=False)` |\n",
    "| R² Score                              | Proportion of variance explained         | `r2_score`                               |\n",
    "\n",
    "**Evaluation metrics for classification (Examples)**\n",
    "| **Metric**               | **Description**                                      | **scikit-learn Function** |\n",
    "| ------------------------ | ---------------------------------------------------- | ------------------------- |\n",
    "| Accuracy                 | Proportion of correct predictions                    | `accuracy_score`          |\n",
    "| Precision                | TP / (TP + FP) — correctness of positive predictions | `precision_score`         |\n",
    "| Recall (Sensitivity)     | TP / (TP + FN) — coverage of actual positives        | `recall_score`            |\n",
    "| F1 Score                 | Harmonic mean of precision and recall                | `f1_score`                |\n",
    "| Confusion Matrix         | Table of TP, FP, FN, TN counts                       | `confusion_matrix`        |\n",
    "| ROC AUC                  | Area under the ROC curve                             | `roc_auc_score`           |\n",
    "\n",
    "\n",
    "```python\n",
    "# Code template for model evaluation\n",
    "train_result = EVAL_METRIC(y_train, y_train_pred)  # You should import `EVAL_METRIC` first.\n",
    "test_result = EVAL_METRIC(y_test, y_test_pred)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e542efc",
   "metadata": {},
   "source": [
    "## 4. Model Selection\n",
    "- Most machine learning models contain hyperparameters, that are not learned from the data, but rather defined by the user before training (e.g., the number of neighbors in KNN, the regularization strength in logistic regression).\n",
    "- The goal of model selection is to find the hyperparameter values that result in the best performance on unseen data.\n",
    "- Since the test set must remain untouched until the final evaluation, we split the training set further into a training and validation subset, or use cross-validation to assess model performance more reliably.\n",
    "- We focus on `GridsearchCV` in `scikit-learn`.\n",
    "\n",
    "```python\n",
    "# Code template for GridsearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "model = ModelName(...)                       # Here, you should not assign hyperparameters to be chosen.\n",
    "param_grid = {\n",
    "    PARAMETER_1: CANDIDATE_1,                # Candidate should be a list\n",
    "    PARAMETER_2: CANDIDATE_2,\n",
    "    ...\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,                                    # 5-fold cross-validation\n",
    "    scoring='scoring_method'                 # See examples below\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_processed, y_train)\n",
    "\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test_processed)\n",
    "\n",
    "## Final evaluation on test set\n",
    "test_score = EVAL_METRIC(y_test, y_pred)     # match this to your scoring above\n",
    "```\n",
    "\n",
    "**`scoring` exmples for regression**\n",
    "| **Metric**                     | **`scoring` String**                       |\n",
    "| ------------------------------ | ------------------------------------------ |\n",
    "| Mean Absolute Error (MAE)      | `'neg_mean_absolute_error'`                |\n",
    "| Mean Squared Error (MSE)       | `'neg_mean_squared_error'`                 |\n",
    "| Root Mean Squared Error (RMSE) | `'neg_root_mean_squared_error'`            |\n",
    "| R² Score                       | `'r2'`                                     |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**`scoring` exmples for classification**\n",
    "| **Metric**           | **`scoring` String** |\n",
    "| -------------------- | -------------------- |\n",
    "| Accuracy             | `'accuracy'`         |\n",
    "| Precision            | `'precision'`        |\n",
    "| Recall               | `'recall'`           |\n",
    "| F1 Score             | `'f1'`               |\n",
    "| ROC AUC              | `'roc_auc'`          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e44f26a",
   "metadata": {},
   "source": [
    "## 5. Save\n",
    "You've finished. Now, it's time to save the model so that we can load anytime we want. We use `pickle`. If you plan to build multiple models, then save the best model under the specific model name.\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "with open('models/ModelName/model.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_search.best_estimator_, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3eaa4f",
   "metadata": {},
   "source": [
    "## 6. Load\n",
    "You can load the model you saved as follows:\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "with open('models/ModelName/model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e409236f",
   "metadata": {},
   "source": [
    "## Next?\n",
    "- Interprete the results.\n",
    "- Visualize the results.\n",
    "- Communicate findings.\n",
    "- Try other models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
