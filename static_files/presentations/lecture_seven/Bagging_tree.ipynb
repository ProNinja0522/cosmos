{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2244fe9f",
   "metadata": {},
   "source": [
    "# Bagging "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578f469a",
   "metadata": {},
   "source": [
    "## 📖 What Is Bagging?\n",
    "\n",
    "Bagging—short for **bootstrap aggregating**—is an **ensemble** method that trains many independent copies of the same base learner on different bootstrap samples (random samples *with* replacement) and then averages their predictions.\n",
    "\n",
    "The idea: **reduce variance** without increasing bias by “voting” across diverse models that each see slightly different data.\n",
    "\n",
    "---\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Conceptual algorithm: \n",
    "1. **Bootstrap** the training set $B$ times → $B$ equally-sized samples **with replacement**.\n",
    "2. Train an **independent** (usually deep) tree on each sample.\n",
    "3. **Aggregate** using majority vote for classification and mean/median for regression.\n",
    "4. **Model selection** (or parameter tuning) using **out-of-bag** (OOB) errors.\n",
    "\n",
    "---\n",
    "\n",
    "### Why It Works\n",
    "\n",
    "\n",
    "* Each tree “sees” a *different* slice of data → their errors are less correlated.\n",
    "* Averaging cancels out noise ⇒ **lower variance** and better generalization.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Popular Flavors\n",
    "\n",
    "| Ensemble                                     | Extra Twist                                                                                 |\n",
    "| -------------------------------------------- | ------------------------------------------------------------------------------------------- |\n",
    "| **Plain BaggingClassifier / Regressor**      | Same features for every tree; randomness only from bootstrapped rows.                       |\n",
    "| **Random Forest**                            | Adds column-sampling at every *split* to further de-correlate trees.                        |\n",
    "| **Extra Trees (Extremely Randomized Trees)** | Uses the whole dataset but selects split thresholds at random—high bias, very low variance. |\n",
    "\n",
    "---\n",
    "\n",
    "### ⚠️ Pros and Cons\n",
    "\n",
    "✅ **Parallel-friendly**: each model trains independently—easy to distribute.  \n",
    "✅ Dramatically **reduces variance** of high-variance learners (e.g.\\ full-depth trees).  \n",
    "✅ Mitigates overfitting without heavy hyper-parameter tuning.  \n",
    "✅ Naturally provides **out-of-bag** error estimates—no extra validation set needed.  \n",
    "\n",
    "❌ Yields **large, memory-hungry** ensembles (hundreds of trees).  \n",
    "❌ Adds little benefit if the base learner is already low-variance (e.g.\\ regularized linear models).  \n",
    "❌ Final model loses interpretability of a single tree; feature importance is more diffuse.  \n",
    "\n",
    "---\n",
    "\n",
    "Key Hyper-parametersa\n",
    "\n",
    "| Symbol in `sklearn` | Meaning                          | Typical starter value |\n",
    "| ------------------- | -------------------------------- | --------------------- |\n",
    "| `n_estimators`      | number of bootstrapped trees $B$ | 100                   |\n",
    "| `max_samples`       | rows per bootstrap sample        | 0.6 – 1.0             |\n",
    "| `max_features`      | columns sent to each tree        | 1.0 (all)             |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8668ac3",
   "metadata": {},
   "source": [
    "> *Think of Bagging as crowd-sourcing your model: each member is noisy, but their average is calm and reliable.* --- Words of wisdom by ChatGPT 3o (again)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5acdd97",
   "metadata": {},
   "source": [
    "\n",
    "#### 3.3  Bagging (Bootstrap Aggregation)\n",
    "\n",
    "\n",
    "Why it works: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c7bb56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)   \n",
    "# Examples with more data points:\n",
    "sample_size = 200\n",
    "X = rng.uniform(0.1, 0.9, size=(sample_size, 2))\n",
    "y = np.zeros(sample_size, dtype=int)\n",
    "mask1 = X[:, 0] + X[:, 1] > 1.1\n",
    "mask2 = (~mask1) & (X[:, 0] - X[:, 1] > 0.3)\n",
    "y[mask1] = 1\n",
    "y[mask2] = 0\n",
    "y[~(mask1 | mask2)] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e5cec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ea8b708a3ab457693448146266ae712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='max_features (≤1.0)', max=1.0, min=0.2), IntSlider(v…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- imports -------------------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import sklearn                                   # version check\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "label_cmap = ListedColormap([\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n",
    "\n",
    "# ----- helper: fit & plot a bagging ensemble --------------------------------\n",
    "def plot_bagging(max_features=1.0, n_estimators=100, max_depth=3):\n",
    "    depth = None if max_depth == 0 else int(max_depth)\n",
    "\n",
    "    # build BaggingClassifier with proper keyword for your scikit-learn version\n",
    "    bag_kwargs = dict(\n",
    "        n_estimators=int(n_estimators),\n",
    "        max_features=max_features,\n",
    "        bootstrap=True,\n",
    "        oob_score=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "    tree = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
    "    if sklearn.__version__ >= \"1.4\":\n",
    "        bag_kwargs[\"estimator\"] = tree\n",
    "    else:                                      # pre-1.4 API\n",
    "        bag_kwargs[\"base_estimator\"] = tree\n",
    "\n",
    "    bag = BaggingClassifier(**bag_kwargs).fit(X, y)\n",
    "\n",
    "    # mesh grid over feature space\n",
    "    x_min, x_max = X[:, 0].min() - 0.05, X[:, 0].max() + 0.05\n",
    "    y_min, y_max = X[:, 1].min() - 0.05, X[:, 1].max() + 0.05\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 400),\n",
    "        np.linspace(y_min, y_max, 400),\n",
    "    )\n",
    "    Z = bag.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # plot decision regions + data\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.25, cmap=label_cmap)\n",
    "    plt.scatter(X[:, 0], X[:, 1],\n",
    "                c=y, cmap=label_cmap,\n",
    "                edgecolors=\"k\", s=80)\n",
    "\n",
    "    depth_label = \"None\" if depth is None else depth\n",
    "    plt.title(f\"Bagging (Decision Trees): max_features={max_features:.2f}, \"\n",
    "              f\"n_estimators={n_estimators}, max_depth={depth_label}\\n\"\n",
    "              f\"OOB accuracy ≈ {bag.oob_score_:.3f}\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ----- interactive widgets --------------------------------------------------\n",
    "interact(\n",
    "    plot_bagging,\n",
    "    max_features=widgets.FloatSlider(\n",
    "        value=1.0, min=0.2, max=1.0, step=0.1,\n",
    "        description=\"max_features (≤1.0)\"\n",
    "    ),\n",
    "    n_estimators=widgets.IntSlider(\n",
    "        value=100, min=10, max=300, step=10,\n",
    "        description=\"n_estimators\"\n",
    "    ),\n",
    "    max_depth=widgets.IntSlider(\n",
    "        value=3, min=0, max=6, step=1,\n",
    "        description=\"max_depth (0=unlimited)\"\n",
    "    ),\n",
    ");\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
