{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fff336d",
   "metadata": {},
   "source": [
    "# Random Forest \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e7ad3a",
   "metadata": {},
   "source": [
    "> In previous lectures, we have seen "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8971bc",
   "metadata": {},
   "source": [
    "\n",
    "## 📖 What Is Random Forest Regression?\n",
    "\n",
    "**Random Forest Regression** is an ensemble learning method that fits many decision trees and averages their predictions.\n",
    "\n",
    "It combines:\n",
    "- **Bagging (Bootstrap Aggregation)**: Fit each tree on a random subset of the data\n",
    "- **Random Feature Subsets**: At each split, choose from a random subset of features\n",
    "\n",
    "---\n",
    "\n",
    "### ⚙️ Key Features\n",
    "\n",
    "- Reduces overfitting of single decision trees\n",
    "- Handles nonlinear relationships well\n",
    "- Provides built-in **feature importance**\n",
    "\n",
    "\n",
    "### 📊 Key Hyperparameters\n",
    "\n",
    "| Parameter | Meaning |\n",
    "|-----------|---------|\n",
    "| `n_estimators` | Number of trees in the forest |\n",
    "| `max_depth` | Maximum depth of each tree |\n",
    "| `max_features` | Number of features considered at each split |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b18654d",
   "metadata": {},
   "source": [
    "> In this note, we will re-examine random forest to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c70d4a6",
   "metadata": {},
   "source": [
    "Now that we have discussed Bagging, we can write down the following equations:\n",
    "\n",
    "Random Forest = **Bagging** + **Random Subspace Method**\n",
    "\n",
    "At *every split* inside each bootstrapped tree, choose only $k$ of the $p$ features ($k < p$) as candidate split columns. This decorrelates trees even if a few predictors are overwhelmingly strong.\n",
    "\n",
    "Why it Works\n",
    "\n",
    "* Like bagging, averaging reduces variance.\n",
    "* Extra feature randomness prevents the “same strong feature first” syndrome, yielding **even lower correlation** between trees.\n",
    "  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "812ef983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)   \n",
    "# Examples with more data points:\n",
    "sample_size = 200\n",
    "X = rng.uniform(0.1, 0.9, size=(sample_size, 2))\n",
    "y = np.zeros(sample_size, dtype=int)\n",
    "mask1 = X[:, 0] + X[:, 1] > 1.1\n",
    "mask2 = (~mask1) & (X[:, 0] - X[:, 1] > 0.3)\n",
    "y[mask1] = 1\n",
    "y[mask2] = 0\n",
    "y[~(mask1 | mask2)] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f79f68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b45744acacc4f039a42109d5b967e65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='max_features', max=1.0, min=0.2), IntSlider(value=10…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ----- imports -------------------------------------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "import sklearn                                   # version check\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "label_cmap = ListedColormap([\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n",
    "\n",
    "# 2 -- helper to fit & plot a forest for the current slider values\n",
    "def plot_rf(max_features=1.0, n_estimators=100, max_depth=0):\n",
    "    depth = None if max_depth == 0 else int(max_depth)\n",
    "\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=int(n_estimators),\n",
    "        max_features=max_features,\n",
    "        max_depth=depth,\n",
    "        oob_score=True,\n",
    "        random_state=42\n",
    "    ).fit(X, y)\n",
    "\n",
    "    # mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 0.05, X[:, 0].max() + 0.05\n",
    "    y_min, y_max = X[:, 1].min() - 0.05, X[:, 1].max() + 0.05\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 400),\n",
    "        np.linspace(y_min, y_max, 400)\n",
    "    )\n",
    "    Z = rf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.25, cmap=label_cmap)\n",
    "    plt.scatter(X[:, 0], X[:, 1],\n",
    "                c=y, cmap=label_cmap,\n",
    "                edgecolors=\"k\", s=80)\n",
    "\n",
    "    depth_label = \"None\" if depth is None else depth\n",
    "    plt.title(f\"Random Forest: max_features={max_features:.2f}, \"\n",
    "              f\"n_estimators={n_estimators}, max_depth={depth_label}\\n\"\n",
    "              f\"OOB accuracy ≈ {rf.oob_score_:.3f}\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3 -- build the widgets\n",
    "interact(\n",
    "    plot_rf,\n",
    "    max_features = widgets.FloatSlider(\n",
    "        value=1.0, min=0.2, max=1.0, step=0.1,\n",
    "        description=\"max_features\"\n",
    "    ),\n",
    "    n_estimators = widgets.IntSlider(\n",
    "        value=100, min=10, max=300, step=10,\n",
    "        description=\"n_estimators\"\n",
    "    ),\n",
    "    max_depth = widgets.IntSlider(\n",
    "        value=0, min=0, max=6, step=1,\n",
    "        description=\"max_depth (0=None)\"\n",
    "    )\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bbf615",
   "metadata": {},
   "source": [
    "> Check out the (slightly overly complicated but fun) visualization by MLU:  https://mlu-explain.github.io/random-forest/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
