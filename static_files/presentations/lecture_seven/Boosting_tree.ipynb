{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0573b0c7",
   "metadata": {},
   "source": [
    "# Boosting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4ce1c3",
   "metadata": {},
   "source": [
    "## What Is Boosting?\n",
    "\n",
    "**Boosting** is an *ensemble* technique that combines many weak predictive models—typically shallow decision trees—into one strong learner.\n",
    "\n",
    "By fitting models **sequentially** and letting each new model focus on the errors of the previous ones, Boosting steadily reduces bias and improves overall accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "### How It Works\n",
    "\n",
    "Conceptual algorithm (Adaptive Boosting, or AdaBoost):\n",
    "\n",
    "1. Start with **uniform** observation weights.\n",
    "2. Train a stump, i.e., a one-level tree (refered to as a weak learner in adaBoost).\n",
    "3. **Up-weight** the rows the stump misclassified.\n",
    "4. Train the next stump on the re-weighted data.\n",
    "5. **Combine** trees in a weighted vote (later trees usually get higher weight).\n",
    "6. Repeat for $M$ rounds.\n",
    "\n",
    "---\n",
    "\n",
    "### Why it Works\n",
    "\n",
    "* Each tree focuses on what its predecessors missed ⇒ **reduces bias**.\n",
    "* Shallow trees + small learning rate keep variance in check.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Popular Boosting methods\n",
    "\n",
    "| Algorithm                         | Core Idea                                                                                             |\n",
    "| --------------------------------- | ----------------------------------------------------------------------------------------------------- |\n",
    "| **AdaBoost**                      | Adjust sample weights via exponential loss; emphasize hard cases.                                     |\n",
    "| **Gradient Boosting**             | Fit each learner to the *negative gradient* of a differentiable loss (e.g., squared, log-loss).       |\n",
    "| **XGBoost / LightGBM / CatBoost** | Engineering-heavy gradient boosting variants with tree pruning, regularization, and GPU acceleration. |\n",
    "\n",
    "---\n",
    "\n",
    "### Pros and Cons\n",
    "\n",
    "✅ Often delivers *state-of-the-art* accuracy out-of-the-box.  \n",
    "✅ **Reduces bias** by turning weak learners into a strong composite.  \n",
    "✅ Handles **mixed feature types** and missing values (tree-based versions).  \n",
    "✅ Offers many regularization knobs (learning rate, subsampling) to fight overfitting.  \n",
    "\n",
    "❌ **Sequential training** means poor parallelism and longer training times.  \n",
    "❌ Can *overfit* if the ensemble grows too deep or learning rate is too high.  \n",
    "❌ Model is less interpretable than a single tree (though feature importance helps).  \n",
    "\n",
    "### Key Hyper-parameters\n",
    "\n",
    "| Symbol                         | Meaning             | Tips                                          |\n",
    "| ------------------------------ | ------------------- | --------------------------------------------- |\n",
    "| `n_estimators`                 | rounds $M$          | 100–500 common                                |\n",
    "| `learning_rate`                | shrinkage per round | 0.01–0.1 (smaller = safer, needs more rounds) |\n",
    "| `max_depth` / `max_leaf_nodes` | size of each tree   | 2–6 for tabular                               |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a08fa57",
   "metadata": {},
   "source": [
    "> *Boosting “learns from its mistakes,” layering many simple models into a powerful predictor that often outperforms standalone learners—provided you tune it with care.*  --- Words of wisdom by ChatGPT 3o"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78643909",
   "metadata": {},
   "source": [
    "We will examine the same simulated example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc0ef58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)   \n",
    "# Examples with more data points:\n",
    "sample_size = 200\n",
    "X = rng.uniform(0.1, 0.9, size=(sample_size, 2))\n",
    "y = np.zeros(sample_size, dtype=int)\n",
    "mask1 = X[:, 0] + X[:, 1] > 1.1\n",
    "mask2 = (~mask1) & (X[:, 0] - X[:, 1] > 0.3)\n",
    "y[mask1] = 1\n",
    "y[mask2] = 0\n",
    "y[~(mask1 | mask2)] = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfefb077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15e74c4f7f3f4a0eb4c3a295cf7a9c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='max_depth', max=4, min=1), IntSlider(value=100, descript…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- numerical & plotting -------------------------------------------------\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap   # only if you still need to define label_cmap\n",
    "# --- machine learning -----------------------------------------------------\n",
    "import sklearn                                # to check sklearn.__version__\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# --- widgets / interactivity ---------------------------------------------\n",
    "import ipywidgets as widgets                  # IntSlider\n",
    "from ipywidgets import interact               # high-level helper\n",
    "\n",
    "\n",
    "label_cmap = ListedColormap([\"#1f77b4\", \"#ff7f0e\", \"#2ca02c\"])\n",
    "\n",
    "\n",
    "def plot_adaboost(max_depth=1, n_estimators=100):\n",
    "    # Build AdaBoost with the right keyword for your scikit-learn version\n",
    "    ada_kwargs = dict(\n",
    "        n_estimators=n_estimators,\n",
    "        learning_rate=0.5,\n",
    "        random_state=42,\n",
    "    )\n",
    "    if sklearn.__version__ >= \"1.4\":\n",
    "        ada_kwargs[\"estimator\"] = DecisionTreeClassifier(max_depth=max_depth)\n",
    "    else:\n",
    "        ada_kwargs[\"base_estimator\"] = DecisionTreeClassifier(max_depth=max_depth)\n",
    "\n",
    "    ada = AdaBoostClassifier(**ada_kwargs)\n",
    "    ada.fit(X, y)\n",
    "\n",
    "    # Create a fine mesh over the feature space\n",
    "    x_min, x_max = X[:, 0].min() - 0.05, X[:, 0].max() + 0.05\n",
    "    y_min, y_max = X[:, 1].min() - 0.05, X[:, 1].max() + 0.05\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 400),\n",
    "        np.linspace(y_min, y_max, 400)\n",
    "    )\n",
    "    Z = ada.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.25, cmap=label_cmap)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=label_cmap,\n",
    "                edgecolors=\"k\", s=80)\n",
    "    plt.title(f\"AdaBoost: max_depth = {max_depth}, \"\n",
    "              f\"n_estimators = {n_estimators}\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.xlim(x_min, x_max)\n",
    "    plt.ylim(y_min, y_max)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. ── interactive sliders (depth 1-4, trees 100-500) ──────────────────\n",
    "interact(\n",
    "    plot_adaboost,\n",
    "    max_depth=widgets.IntSlider(min=1, max=4, step=1, value=1,\n",
    "                                description=\"max_depth\"),\n",
    "    n_estimators=widgets.IntSlider(min=100, max=500, step=100, value=100,\n",
    "                                   description=\"n_estimators\")\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064c57fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
